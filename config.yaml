ollama:
  base_url: "http://localhost:11434"
  model: "llama3" 
  temperature: 0.7
  timeout: 120

logging:
  level: "INFO"
  format: "%(asctime)s - %(name)s - %(levelname)s - %(message)s"

experiment:
  timeout: 3600
  max_retries: 2
  use_venv: true
  random_seed: 42 
  set_environment_seeds: true 

evaluation:
  threshold: 0.05 
  metrics_of_interest:
    - accuracy
    - f1_score
    - precision
    - recall
    - loss

paths:
  temp_dir: "./temp"
  output_dir: "./outputs"
  cache_dir: "./.cache"
