import sys
from pathlib import Path

# Add src to path
sys.path.insert(0, str(Path(__file__).parent / 'src'))

from main import ReproductionAgent


def main():
    """Run agent with auto-detection."""
    print("="*70)
    print("ğŸ”¬ LOCAL RESEARCH PAPER REPRODUCTION AGENT")
    print("="*70)
    print("\nAuto-detecting paper and codebase from workspace...")
    print("\nExpected structure:")
    print("  ğŸ“‚ paper/                    # Contains PDF paper")
    print("  ğŸ“‚ paper_source_code/")
    print("      â””â”€â”€ supplementary_material/")
    print("          â”œâ”€â”€ code/            # Main code")
    print("          â”œâ”€â”€ idea_generation/")
    print("          â””â”€â”€ references/")
    print("="*70)
    
    # Check workspace structure
    workspace_root = Path(__file__).parent
    paper_dir = workspace_root / "paper"
    code_dir = workspace_root / "paper_source_code" / "supplementary_material"
    
    print("\nğŸ“ Checking workspace structure...")
    
    if not paper_dir.exists():
        print(f"âŒ ./paper/ directory not found!")
        print("   Please create it and add your research paper PDF")
        return 1
    
    pdf_files = list(paper_dir.glob("*.pdf"))
    if not pdf_files:
        print(f"âŒ No PDF files found in ./paper/")
        print("   Please add your research paper PDF to ./paper/")
        return 1
    
    print(f"âœ“ Found paper: {pdf_files[0].name}")
    
    if not code_dir.exists():
        print(f"âš ï¸  ./paper_source_code/supplementary_material/ not found!")
        print("   Will search for GitHub URLs in paper...")
    else:
        print(f"âœ“ Found code directory: {code_dir}")
    
    # Check Ollama
    print("\nğŸ” Checking Ollama...")
    from llm_client import OllamaClient
    client = OllamaClient()
    
    if not client.is_available():
        print("âŒ Ollama is not running!")
        print("\n   Please start Ollama in another terminal:")
        print("   $ ollama serve")
        print("\n   And ensure you have a model installed:")
        print("   $ ollama pull llama3")
        return 1
    
    models = client.list_models()
    if not models:
        print("âŒ No Ollama models found!")
        print("\n   Please pull a model:")
        print("   $ ollama pull llama3")
        return 1
    
    print(f"âœ“ Ollama is running (models: {', '.join(models[:3])})")
    
    # Run agent
    print("\n" + "="*70)
    print("ğŸš€ Starting reproduction workflow...")
    print("="*70 + "\n")
    
    try:
        agent = ReproductionAgent()
        results = agent.run()  # No arguments - auto-detect!
        
        if 'error' in results:
            print(f"\nâŒ Error: {results['error']}")
            return 1
        
        print("\n" + "="*70)
        print("âœ… WORKFLOW COMPLETED!")
        print("="*70)
        print(f"\nğŸ“Š Check outputs/ directory for results")
        
        return 0
        
    except KeyboardInterrupt:
        print("\n\nâš ï¸  Interrupted by user")
        return 1
    except Exception as e:
        print(f"\nâŒ Error: {e}")
        import traceback
        traceback.print_exc()
        return 1


if __name__ == '__main__':
    sys.exit(main())
