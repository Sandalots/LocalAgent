"""===============================================================================
EVALLAB PIPELINE ORCHESTRATOR (PIPELINES THE 4 STAGES)

Coordinates all 4 stages: Paper Parsing ‚Üí Code Retrieval ‚Üí Experiment Execution ‚Üí Result Evaluation.
===============================================================================
"""

import logging
import sys
import time
from pathlib import Path
from typing import Optional, Dict, List, Any
import yaml
from dotenv import load_dotenv
import os
import json
import re
import requests

from src.paper_parser import PaperParser, PaperContent
from src.repo_retriever import RepoRetriever
from src.experiment_executor import ExperimentExecutor, CodebaseInfo, ExperimentConfig
from src.result_evaluator import ResultEvaluator

# Custom colored logging formatter


class ColoredFormatter(logging.Formatter):
    """Custom formatter with colors for different log levels and highlights for special content."""
    # ANSI color codes
    COLORS = {
        'DEBUG': '\033[36m',      # Cyan
        'INFO': '\033[92m',       # Green
        'WARNING': '\033[93m',    # Yellow
        'ERROR': '\033[91m',      # Red
        'CRITICAL': '\033[95m',   # Magenta
    }
    RESET = '\033[0m'

    # Color for different parts
    TIME_COLOR = '\033[90m'       # Gray
    NAME_COLOR = '\033[94m'       # Blue
    LEVEL_COLORS = COLORS

    # Highlight colors for special content
    NUMBER_COLOR = '\033[96m'     # Cyan - for numbers
    PATH_COLOR = '\033[93m'       # Yellow - for file paths
    METRIC_COLOR = '\033[95m'     # Magenta - for percentages/metrics
    # Red - for ML metric names (F1, accuracy, etc.)
    ML_METRIC_COLOR = '\033[91m'
    KEYWORD_COLOR = '\033[93m'    # Bright yellow - for special keywords
    # Green - for status words (success, pass, etc.)
    STATUS_COLOR = '\033[92m'

    def colorize_message(self, message, base_color):
        """Add special highlighting to numbers, paths, and metrics in the message."""
        import re

        # Highlight status words in green (success, pass, completed, etc.)
        message = re.sub(
            r'\b(success|pass|passed|completed?|excellent|good|failed?|error)\b',
            f'{self.STATUS_COLOR}\\1{base_color}',
            message,
            flags=re.IGNORECASE
        )

        # Highlight special keywords in bright yellow (ollama, baseline, reproduced, etc.)
        message = re.sub(
            r'\b(ollama|llama3|baseline|reproduced?|reproduction|grade|assessment|success\s+rate|avg\s+deviation|average\s+deviation|mean\s+deviation|avg\s+diff|deviation|difference|diff|metric|method|experiment|stage|pipeline|configuration|granularity|retrieval|downstream|model)\b',
            f'{self.KEYWORD_COLOR}\\1{base_color}',
            message,
            flags=re.IGNORECASE
        )

        # Highlight ML metric names in red (F1, accuracy, recall, precision, etc.)
        message = re.sub(
            r'\b(f1|accuracy|recall|precision|mrr|specificity|sensitivity|auc|roc|mse|mae|rmse|r2|loss|score)[-_\s]?(score|value|rate)?s?\b',
            f'{self.ML_METRIC_COLOR}\\1\\2{base_color}',
            message,
            flags=re.IGNORECASE
        )

        # Highlight percentages and metrics with units (e.g., 94.2%, 0.05, 10.5MB, 2.5s, etc.)
        message = re.sub(
            r'(\d+\.?\d*%|\d+\.\d+(?:MB|GB|KB|s|ms|mins?|hours?)?|\d+(?:MB|GB|KB|s|ms|mins?|hours?))',
            f'{self.METRIC_COLOR}\\1{base_color}',
            message
        )

        # Highlight file paths (anything with / or .py, .txt, .json, etc.)
        message = re.sub(
            r'([/\w\-\.]+/[\w\-\./]+|[\w\-]+\.(py|txt|json|yaml|md|csv|html|pdf))',
            f'{self.PATH_COLOR}\\1{base_color}',
            message
        )

        # Highlight standalone numbers (not already colored by previous rules)
        # Avoid re-coloring numbers that are part of paths or metrics
        message = re.sub(
            r'(?<![/\w\-\.%])\b(\d+)\b(?![/\w\-\.%MGKB])',
            f'{self.NUMBER_COLOR}\\1{base_color}',
            message
        )

        return message

    def format(self, record):
        # Color the timestamp
        timestamp = self.formatTime(record, self.datefmt)
        colored_time = f"{self.TIME_COLOR}{timestamp}{self.RESET}"

        # Color the logger name
        colored_name = f"{self.NAME_COLOR}{record.name}{self.RESET}"

        # Color the level name
        level_color = self.LEVEL_COLORS.get(record.levelname, self.RESET)
        colored_level = f"{level_color}{record.levelname}{self.RESET}"

        # Get base message color
        message_color = self.LEVEL_COLORS.get(record.levelname, self.RESET)

        # Colorize the message with special highlights
        message = record.getMessage()
        colored_message = f"{message_color}{self.colorize_message(message, message_color)}{self.RESET}"

        return f"{colored_time} - {colored_name} - {colored_level} - {colored_message}"


# Set up dual logging: colored console + plain file
# Console handler with colors
console_handler = logging.StreamHandler()
console_handler.setFormatter(ColoredFormatter(datefmt='%Y-%m-%d %H:%M:%S'))

# File handler without colors (plain text for log file)
log_file_path = Path('./outputs/agent_execution.log')
log_file_path.parent.mkdir(exist_ok=True, parents=True)
file_handler = logging.FileHandler(log_file_path, mode='w', encoding='utf-8')
file_handler.setFormatter(logging.Formatter(
    '%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S'
))

logging.basicConfig(
    level=logging.INFO,
    handlers=[console_handler, file_handler]
)
logger = logging.getLogger(__name__)


class ReproductionAgent:
    """Main agent that coordinates paper reproduction workflow with integrated LLM."""

    def __init__(self, config_path: Optional[Path] = None):
        """
        Initialize the reproduction agent.

        Args:
            config_path: Optional path to config.yaml file
        """
        # Load environment variables
        load_dotenv()

        # Load configuration
        self.config = self._load_config(config_path)

        # Initialize LLM settings (integrated from llm_client)
        self.ollama_base_url = self.config['ollama']['base_url']
        self.ollama_model = self.config['ollama']['model']
        self.ollama_temperature = self.config['ollama']['temperature']
        self.ollama_timeout = self.config['ollama']['timeout']

        # Initialize components (using new 4-stage architecture)
        self.paper_parser = PaperParser()
        self.repo_retriever = RepoRetriever()
        # Extract paper name for per-paper logging
        paper_name = None
        if hasattr(self, 'paper_path') and self.paper_path:
            paper_name = Path(self.paper_path).stem
        elif 'paper' in self.config and self.config['paper']:
            paper_name = Path(self.config['paper']).stem
        self.experiment_executor = ExperimentExecutor(
            config=self.config, paper_name=paper_name)
        self.result_evaluator = ResultEvaluator(
            llm_client=self,  # Pass self as we have integrated LLM methods
            threshold=self.config['evaluation']['threshold']
        )

        # Create output directory
        self.output_dir = Path(self.config['paths']['output_dir'])
        self.output_dir.mkdir(exist_ok=True, parents=True)

    def _load_config(self, config_path: Optional[Path]) -> dict:
        """Load configuration from YAML file."""
        if config_path is None:
            config_path = Path(__file__).parent.parent / 'config.yaml'

        if not config_path.exists():
            logger.warning(
                f"Config file not found: {config_path}, using defaults")
            return self._default_config()

        with open(config_path, 'r') as f:
            return yaml.safe_load(f)

    def _default_config(self) -> dict:
        """Return default configuration."""
        return {
            'ollama': {
                'base_url': os.getenv('OLLAMA_BASE_URL', 'http://localhost:11434'),
                'model': os.getenv('OLLAMA_MODEL', 'llama3'),
                'temperature': float(os.getenv('OLLAMA_TEMPERATURE', '0.7')),
                'timeout': int(os.getenv('OLLAMA_TIMEOUT', '120'))
            },
            'logging': {'level': 'INFO'},
            'experiment': {'timeout': 3600, 'max_retries': 2, 'use_venv': True},
            'evaluation': {'threshold': 0.05},
            'paths': {
                'output_dir': './outputs'
            }
        }

    # ============================================================================
    # INTEGRATED OLLAMA LLM CLIENT METHODS
    # ============================================================================

    def is_available(self) -> bool:
        """Check if Ollama is running and accessible."""
        try:
            response = requests.get(
                f"{self.ollama_base_url}/api/tags", timeout=5)
            return response.status_code == 200
        except requests.exceptions.RequestException:
            return False

    def list_models(self) -> List[str]:
        """List available models in Ollama."""
        try:
            response = requests.get(
                f"{self.ollama_base_url}/api/tags", timeout=5)
            response.raise_for_status()
            data = response.json()
            return [model['name'] for model in data.get('models', [])]
        except requests.exceptions.RequestException as e:
            logger.error(f"Error listing models: {e}")
            return []

    def generate(self, prompt: str, system_prompt: Optional[str] = None,
                 temperature: Optional[float] = None) -> str:
        """
        Generate a response from the LLM.

        Args:
            prompt: User prompt
            system_prompt: Optional system prompt for context
            temperature: Optional temperature override

        Returns:
            Generated text response
        """
        url = f"{self.ollama_base_url}/api/generate"

        payload = {
            "model": self.ollama_model,
            "prompt": prompt,
            "stream": False,
            "options": {
                "temperature": temperature or self.ollama_temperature
            }
        }

        if system_prompt:
            payload["system"] = system_prompt

        try:
            response = requests.post(
                url,
                json=payload,
                timeout=self.ollama_timeout
            )
            response.raise_for_status()

            result = response.json()
            return result.get('response', '').strip()

        except requests.exceptions.RequestException as e:
            logger.error(f"Error generating response: {e}")
            raise

    def chat(self, messages: List[Dict[str, str]],
             temperature: Optional[float] = None) -> str:
        """
        Chat completion with conversation history.

        Args:
            messages: List of message dicts with 'role' and 'content'
            temperature: Optional temperature override

        Returns:
            Generated response
        """
        url = f"{self.ollama_base_url}/api/chat"

        payload = {
            "model": self.ollama_model,
            "messages": messages,
            "stream": False,
            "options": {
                "temperature": temperature or self.ollama_temperature
            }
        }

        try:
            response = requests.post(
                url,
                json=payload,
                timeout=self.ollama_timeout
            )
            response.raise_for_status()

            result = response.json()
            return result.get('message', {}).get('content', '').strip()

        except requests.exceptions.RequestException as e:
            logger.error(f"Error in chat completion: {e}")
            raise

    def extract_json(self, prompt: str, system_prompt: Optional[str] = None) -> Dict[str, Any]:
        """
        Generate a response and parse it as JSON.

        Args:
            prompt: User prompt
            system_prompt: Optional system prompt

        Returns:
            Parsed JSON dictionary
        """
        import re
        response = self.generate(prompt, system_prompt, temperature=0.1)

        # Try to extract JSON from markdown code blocks first
        json_match = re.search(
            r'```(?:json)?\s*(\{.*?\})\s*```', response, re.DOTALL)
        if json_match:
            response = json_match.group(1)

        # Try to find JSON object in the response
        json_match = re.search(r'\{.*\}', response, re.DOTALL)
        if json_match:
            response = json_match.group(0)

        try:
            # Clean control characters that break JSON
            cleaned_response = re.sub(r'[\x00-\x1F\x7F]', '', response)
            return json.loads(cleaned_response)
        except json.JSONDecodeError as e:
            logger.error(f"Failed to parse JSON: {e}")
            logger.error(f"Full raw LLM response:\n{response}")
            logger.debug(f"Cleaned response:\n{cleaned_response}")
            return {}

    # ============================================================================
    # MAIN WORKFLOW METHODS
    # ============================================================================

    def run(self, paper_path: Optional[Path] = None, codebase_source: Optional[str] = None) -> dict:
        """
        Run the complete reproduction workflow.

        Args:
            paper_path: Path to the research paper PDF (if None, searches ./papers/ directory)
            codebase_source: Either GitHub URL or path to local codebase (if None, searches ./papers/codebases/)

        Returns:
            Dictionary with results and evaluation
        """
        logger.info("="*70)
        logger.info("Starting EVALLab...")
        logger.info("="*70)

        # Auto-detect paper if not provided
        if paper_path is None:
            workspace_root = Path(__file__).parent.parent
            paper_dir = workspace_root / "paper"

            if paper_dir.exists():
                pdf_files = list(paper_dir.glob("*.pdf"))
                if pdf_files:
                    paper_path = pdf_files[0]
                    logger.info(f"‚úì Auto-detected paper: {paper_path.name}")
                else:
                    logger.error("No PDF files found in ./papers/ directory")
                    return {'error': 'No paper PDF found'}
            else:
                logger.error("./papers/ directory not found")
                return {'error': 'No paper directory found'}

        # Step 1: Check Ollama availability
        if not self.is_available():
            logger.error("Ollama is not running or not accessible!")
            logger.error(
                f"Please start Ollama and ensure it's running at {self.ollama_base_url}")
            return {'error': 'Ollama not available'}

        logger.info(f"‚úì Connected to Ollama (model: {self.ollama_model})")

        # Log which paper is being parsed
        logger.info(f"Parsing paper: {paper_path}")

        # Step 2: Parse the paper (Stage 1)
        print("\n" + "="*80)
        print("\033[94m‚îå" + "‚îÄ"*78 + "‚îê\033[0m")
        print("\033[94m‚îÇ\033[0m" +
              "\033[1;96m STAGE 1/4: PAPER PARSING".center(78) + "\033[94m‚îÇ\033[0m")
        print("\033[94m‚îú" + "‚îÄ"*78 + "‚î§\033[0m")
        print("\033[94m‚îÇ\033[0m" +
              " üìÑ Extracting text, figures, and GitHub URLs from PDF".ljust(78) + "\033[94m‚îÇ\033[0m")
        print("\033[94m‚îÇ\033[0m" +
              " üîç Parsing paper structure and metadata".ljust(78) + "\033[94m‚îÇ\033[0m")
        print("\033[94m‚îî" + "‚îÄ"*78 + "‚îò\033[0m")
        print("="*80 + "\n")

        paper_content = self.paper_parser.parse_pdf(paper_path)
        logger.info(
            f"‚úì Extracted {len(paper_content.raw_text)} characters from paper")

        if paper_content.github_urls:
            logger.info(
                f"‚úì Found {len(paper_content.github_urls)} GitHub URLs in paper")
            for url in paper_content.github_urls:
                logger.info(f"  - {url}")

        # Step 3: Extract key sections using LLM
        print("\n" + "="*80)
        print("\033[93m‚îå" + "‚îÄ"*78 + "‚îê\033[0m")
        print("\033[93m‚îÇ\033[0m" +
              "\033[1;93m STAGE 1.5/4: LLM SECTION EXTRACTION".center(78) + "\033[93m‚îÇ\033[0m")
        print("\033[93m‚îú" + "‚îÄ"*78 + "‚î§\033[0m")
        print("\033[93m‚îÇ\033[0m" +
              " ü§ñ Using Ollama LLM to extract key sections".ljust(78) + "\033[93m‚îÇ\033[0m")
        print("\033[93m‚îÇ\033[0m" +
              " üìù Identifying: Abstract, Methodology, Experiments, Results".ljust(78) + "\033[93m‚îÇ\033[0m")
        print("\033[93m‚îî" + "‚îÄ"*78 + "‚îò\033[0m")
        print("="*80 + "\n")

        sections = self._extract_paper_sections(paper_content.raw_text)
        paper_content.abstract = sections.get('abstract', '')
        paper_content.methodology = sections.get('methodology', '')
        paper_content.experiments = sections.get('experiments', '')
        paper_content.results = sections.get('results', '')

        logger.info(
            f"‚úì Extracted abstract ({len(paper_content.abstract)} chars)")
        logger.info(
            f"‚úì Extracted methodology ({len(paper_content.methodology)} chars)")
        logger.info(
            f"‚úì Extracted experiments ({len(paper_content.experiments)} chars)")

        # Step 4: Retrieve codebase (Stage 2 - NEW UNIFIED MODULE)
        print("\n" + "="*80)
        print("\033[92m‚îå" + "‚îÄ"*78 + "‚îê\033[0m")
        print("\033[92m‚îÇ\033[0m" +
              "\033[1;92m STAGE 2/4: CODE RETRIEVAL".center(78) + "\033[92m‚îÇ\033[0m")
        print("\033[92m‚îú" + "‚îÄ"*78 + "‚î§\033[0m")
        print("\033[92m‚îÇ\033[0m" +
              " üîé Priority: User path ‚Üí GitHub (paper-specific) ‚Üí Local dir (fallback)".ljust(78) + "\033[92m‚îÇ\033[0m")
        print("\033[92m‚îÇ\033[0m" +
              " üì¶ Searching for experiment code and dependencies".ljust(78) + "\033[92m‚îÇ\033[0m")
        print("\033[92m‚îî" + "‚îÄ"*78 + "‚îò\033[0m")
        print("="*80 + "\n")

        # Convert codebase_source to Path if it's a string
        local_path = Path(codebase_source) if codebase_source else None

        codebase_path = self.repo_retriever.retrieve_code(
            github_urls=paper_content.github_urls,
            local_path=local_path
        )

        if not codebase_path:
            print("\n" + "="*80)
            print("\033[91m‚îå" + "‚îÄ"*78 + "‚îê\033[0m")
            print("\033[91m‚îÇ\033[0m" +
                  "\033[1;91m ‚ùå NO CODEBASE FOUND".center(78) + "\033[91m‚îÇ\033[0m")
            print("\033[91m‚îú" + "‚îÄ"*78 + "‚î§\033[0m")
            print("\033[91m‚îÇ\033[0m" +
                  " Unable to find experiment code from any source.".ljust(78) + "\033[91m‚îÇ\033[0m")
            print("\033[91m‚îÇ\033[0m" + " ".ljust(78) + "\033[91m‚îÇ\033[0m")
            print("\033[91m‚îÇ\033[0m" +
                  " üí° SOLUTION: Manually add codebase to ./papers/codebases/".ljust(78) + "\033[91m‚îÇ\033[0m")
            print("\033[91m‚îÇ\033[0m" + " ".ljust(78) + "\033[91m‚îÇ\033[0m")
            print("\033[91m‚îÇ\033[0m" +
                  "   Steps:".ljust(78) + "\033[91m‚îÇ\033[0m")
            print("\033[91m‚îÇ\033[0m" +
                  "   1. Create ./papers/codebases/ directory if it doesn't exist".ljust(78) + "\033[91m‚îÇ\033[0m")
            print("\033[91m‚îÇ\033[0m" +
                  "   2. Place your experiment codebase inside it".ljust(78) + "\033[91m‚îÇ\033[0m")
            print("\033[91m‚îÇ\033[0m" +
                  "   3. Run the agent again".ljust(78) + "\033[91m‚îÇ\033[0m")
            print("\033[91m‚îÇ\033[0m" + " ".ljust(78) + "\033[91m‚îÇ\033[0m")
            print("\033[91m‚îÇ\033[0m" +
                  " Checked:".ljust(78) + "\033[91m‚îÇ\033[0m")
            if local_path:
                print(
                    "\033[91m‚îÇ\033[0m" + f"   ‚úó User path: {local_path}".ljust(78) + "\033[91m‚îÇ\033[0m")
            print("\033[91m‚îÇ\033[0m" +
                  "   ‚úó Local directory: ./papers/codebases/".ljust(78) + "\033[91m‚îÇ\033[0m")
            if paper_content.github_urls:
                print("\033[91m‚îÇ\033[0m" + f"   ‚úó GitHub URLs: {len(paper_content.github_urls)} found but failed to clone".ljust(
                    78) + "\033[91m‚îÇ\033[0m")
            else:
                print(
                    "\033[91m‚îÇ\033[0m" + "   ‚úó GitHub URLs: None found in paper".ljust(78) + "\033[91m‚îÇ\033[0m")
            print("\033[91m‚îî" + "‚îÄ"*78 + "‚îò\033[0m")
            print("="*80 + "\n")
            logger.error("No codebase available!")
            return {'error': 'No codebase available - please add code to ./papers/codebases/'}

        logger.info(f"‚úì Codebase retrieved at: {codebase_path}")

        # Step 5: Analyze codebase structure (Stage 3 - UNIFIED ANALYSIS & EXECUTION)
        print("\n" + "="*80)
        print("\033[95m‚îå" + "‚îÄ"*78 + "‚îê\033[0m")
        print("\033[95m‚îÇ\033[0m" +
              "\033[1;95m STAGE 3/4: EXPERIMENT EXECUTION".center(78) + "\033[95m‚îÇ\033[0m")
        print("\033[95m‚îú" + "‚îÄ"*78 + "‚î§\033[0m")
        print("\033[95m‚îÇ\033[0m" +
              " üî¨ Analyzing codebase structure and dependencies".ljust(78) + "\033[95m‚îÇ\033[0m")
        print("\033[95m‚îÇ\033[0m" +
              " ‚öôÔ∏è  Setting up environment and validating data".ljust(78) + "\033[95m‚îÇ\033[0m")
        print("\033[95m‚îÇ\033[0m" +
              " üöÄ Running experiments with reproducible seeds".ljust(78) + "\033[95m‚îÇ\033[0m")
        print("\033[95m‚îî" + "‚îÄ"*78 + "‚îò\033[0m")
        print("="*80 + "\n")

        codebase_info = self.experiment_executor.analyze_codebase(
            codebase_path)
        logger.info(
            f"‚úì Analyzed codebase (language: {codebase_info.language})")
        logger.info(
            f"‚úì Found {len(codebase_info.entry_points)} potential entry points")
        logger.info(f"‚úì Found {len(codebase_info.dependencies)} dependencies")

        # Validate data integrity before running experiments
        logger.info("\n[Stage 3.5/4] Validating data integrity...")
        validation_results = self.experiment_executor.validate_data_integrity(
            codebase_info.path)

        if not validation_results['valid']:
            logger.warning(
                "‚ö†Ô∏è  Data validation failed - experiments may not reproduce correctly")

        if validation_results['file_stats']:
            total_size = sum(s.get('size_mb', 0)
                             for s in validation_results['file_stats'].values())
            logger.info(
                f"‚úì Data validation complete - {len(validation_results['file_stats'])} files, {total_size:.1f}MB total")

        # Run experiments
        logger.info("\n[Stage 3.6/4] Running experiments...")
        experiment_results = self._run_experiments_unified(
            paper_content, codebase_info)

        if not experiment_results:
            logger.error("No experiments were run successfully")
            return {'error': 'Experiment execution failed'}

        logger.info(f"‚úì Completed {len(experiment_results)} experiments")

        # Step 6: Evaluate results (Stage 4)
        print("\n" + "="*80)
        print("\033[96m‚îå" + "‚îÄ"*78 + "‚îê\033[0m")
        print("\033[96m‚îÇ\033[0m" +
              "\033[1;96m STAGE 4/4: RESULT EVALUATION".center(78) + "\033[96m‚îÇ\033[0m")
        print("\033[96m‚îú" + "‚îÄ"*78 + "‚î§\033[0m")
        print("\033[96m‚îÇ\033[0m" +
              " üìä Comparing reproduced metrics to baseline results".ljust(78) + "\033[96m‚îÇ\033[0m")
        print("\033[96m‚îÇ\033[0m" +
              " üìà Generating visualizations and performance reports".ljust(78) + "\033[96m‚îÇ\033[0m")
        print("\033[96m‚îÇ\033[0m" +
              " ‚úÖ Calculating success rate and deviation metrics".ljust(78) + "\033[96m‚îÇ\033[0m")
        print("\033[96m‚îî" + "‚îÄ"*78 + "‚îò\033[0m")
        print("="*80 + "\n")

        # Load ALL experiment results from all output directories
        experiment_sets = self.result_evaluator.load_all_experiment_results(
            codebase_info.path)

        if not experiment_sets:
            logger.error("No experiment result sets found")
            return {'error': 'No experiment results found'}

        logger.info(f"‚úì Loaded {len(experiment_sets)} experiment sets")

        # Extract all metrics from all experiments
        all_reproduced_metrics = self.result_evaluator.extract_all_metrics_from_experiments(
            experiment_sets)
        logger.info(
            f"‚úì Extracted {len(all_reproduced_metrics)} total metrics from all experiments")

        # Try to extract baseline from paper AND report.md files
        baseline = self.result_evaluator.extract_baseline_from_paper(
            paper_content.results or paper_content.raw_text,
            codebase_path=codebase_info.path  # Pass codebase path for report.md parsing
        )
        logger.info(f"‚úì Extracted {len(baseline.metrics)} baseline metrics")
        logger.info(f"  Source: {baseline.source}")

        # Compare all reproduced metrics to baseline
        comparisons = self.result_evaluator.compare_results(
            baseline, all_reproduced_metrics)
        logger.info(f"‚úì Generated {len(comparisons)} metric comparisons")

        # Generate comprehensive report
        report = self.result_evaluator.generate_report(comparisons)
        logger.info("\n" + report)

        # Generate summary statistics
        print("\n" + "="*80)
        print("\033[93m‚îå" + "‚îÄ"*78 + "‚îê\033[0m")
        print("\033[93m‚îÇ\033[0m" +
              "\033[1;93m üìä SUMMARY STATISTICS".center(78) + "\033[93m‚îÇ\033[0m")
        print("\033[93m‚îú" + "‚îÄ"*78 + "‚î§\033[0m")
        print("\033[93m‚îÇ\033[0m" +
              " üìà Performance metrics and success rates".ljust(78) + "\033[93m‚îÇ\033[0m")
        print("\033[93m‚îÇ\033[0m" +
              " üéØ Deviation analysis and accuracy scores".ljust(78) + "\033[93m‚îÇ\033[0m")
        print("\033[93m‚îî" + "‚îÄ"*78 + "‚îò\033[0m")
        print("="*80 + "\n")

        summary_stats = self.result_evaluator.generate_summary_statistics(
            comparisons)
        logger.info(summary_stats)

        # Get LLM analysis of differences
        analysis = ""
        if comparisons:
            print("\n" + "="*80)
            print("\033[94m‚îå" + "‚îÄ"*78 + "‚îê\033[0m")
            print("\033[94m‚îÇ\033[0m" +
                  "\033[1;94m ü§ñ LLM ANALYSIS".center(78) + "\033[94m‚îÇ\033[0m")
            print("\033[94m‚îú" + "‚îÄ"*78 + "‚î§\033[0m")
            print("\033[94m‚îÇ\033[0m" +
                  " üß† AI-powered analysis of result differences".ljust(78) + "\033[94m‚îÇ\033[0m")
            print("\033[94m‚îÇ\033[0m" +
                  " üí° Insights into methodology and experiment variations".ljust(78) + "\033[94m‚îÇ\033[0m")
            print("\033[94m‚îî" + "‚îÄ"*78 + "‚îò\033[0m")
            print("="*80 + "\n")

            analysis = self.result_evaluator.analyze_differences_with_llm(
                comparisons,
                paper_content.methodology + "\n" + paper_content.experiments
            )
            logger.info(analysis)

        # Generate comprehensive conclusions and recommendations
        print("\n" + "="*80)
        print("\033[92m‚îå" + "‚îÄ"*78 + "‚îê\033[0m")
        print("\033[92m‚îÇ\033[0m" +
              "\033[1;92m üìã CONCLUSIONS & RECOMMENDATIONS".center(78) + "\033[92m‚îÇ\033[0m")
        print("\033[92m‚îú" + "‚îÄ"*78 + "‚î§\033[0m")
        print("\033[92m‚îÇ\033[0m" +
              " ‚úÖ Comprehensive analysis of reproduction success".ljust(78) + "\033[92m‚îÇ\033[0m")
        print("\033[92m‚îÇ\033[0m" +
              " üîç Key findings and improvement recommendations".ljust(78) + "\033[92m‚îÇ\033[0m")
        print("\033[92m‚îî" + "‚îÄ"*78 + "‚îò\033[0m")
        print("="*80 + "\n")

        conclusions = self.result_evaluator.generate_comprehensive_conclusions(
            comparisons,
            experiment_sets,
            baseline,
            paper_content.methodology + "\n" + paper_content.experiments
        )
        logger.info(conclusions)

        # Generate visualizations
        print("\n" + "="*80)
        print("\033[95m‚îå" + "‚îÄ"*78 + "‚îê\033[0m")
        print("\033[95m‚îÇ\033[0m" +
              "\033[1;95m üìä VISUALIZATION GENERATION".center(78) + "\033[95m‚îÇ\033[0m")
        print("\033[95m‚îú" + "‚îÄ"*78 + "‚î§\033[0m")
        print("\033[95m‚îÇ\033[0m" +
              " üìà Creating charts: bar, scatter, heatmap, histogram".ljust(78) + "\033[95m‚îÇ\033[0m")
        print("\033[95m‚îÇ\033[0m" +
              " üé® Generating HTML dashboard and CSV exports".ljust(78) + "\033[95m‚îÇ\033[0m")
        print("\033[95m‚îî" + "‚îÄ"*78 + "‚îò\033[0m")
        print("="*80 + "\n")

        try:
            # Save visualizations in per-paper subdirectory
            viz_dir = Path('outputs') / 'visualizations' / paper_path.stem
            paper_name = paper_path.stem  # Get filename without extension
            viz_files = self.result_evaluator.generate_visualizations(
                comparisons,
                output_dir=viz_dir,
                paper_name=paper_name
            )
            logger.info(f"‚úì Generated {len(viz_files)} visualization files")
            logger.info(
                f"üìä View visualizations: {viz_dir / 'visualizations.html'}")

            # Generate top-level dashboard listing all papers
            self.result_evaluator.generate_visualizations_index(
                Path('outputs') / 'visualizations')
        except Exception as e:
            logger.error(f"Failed to generate visualizations: {e}")
            import traceback
            traceback.print_exc()

        # Save results
        self._save_results(paper_path, comparisons, report, summary_stats,
                           analysis, conclusions, experiment_sets)

        print("\n" + "="*80)
        print("\033[92m‚îå" + "‚îÄ"*78 + "‚îê\033[0m")
        print("\033[92m‚îÇ\033[0m" +
              "\033[1;92m ‚úÖ WORKFLOW COMPLETED SUCCESSFULLY!".center(78) + "\033[92m‚îÇ\033[0m")
        print("\033[92m‚îú" + "‚îÄ"*78 + "‚î§\033[0m")
        print("\033[92m‚îÇ\033[0m" +
              " üéâ All 4 stages completed".ljust(78) + "\033[92m‚îÇ\033[0m")
        print("\033[92m‚îÇ\033[0m" +
              " üìÅ Results saved to outputs/ directory".ljust(78) + "\033[92m‚îÇ\033[0m")
        print("\033[92m‚îÇ\033[0m" +
              " üìä Visualizations available in outputs/visualizations/".ljust(78) + "\033[92m‚îÇ\033[0m")
        print("\033[92m‚îî" + "‚îÄ"*78 + "‚îò\033[0m")
        print("="*80 + "\n")

        return {
            'paper_content': paper_content,
            'codebase_info': codebase_info,
            'experiment_results': experiment_results,
            'baseline_metrics': baseline,
            'comparisons': comparisons,
            'report': report,
            'conclusions': conclusions
        }

    def _extract_paper_sections(self, raw_text: str) -> dict:
        """Use LLM to extract key sections from paper."""
        system_prompt = (
            "You are an expert at reading research papers. "
            "Extract the following sections: abstract, methodology, experiments, and results. "
            "Return ONLY valid JSON with these exact keys: 'abstract', 'methodology', 'experiments', 'results'. "
            "Do NOT use a 'sections' array or any other structure. "
            "Each value should be a full, detailed paragraph or set of paragraphs, not a summary. "
            "Do not truncate or omit information. "
            "If a section is long, include as much as possible. "
            "Do not include markdown, explanations, or any text outside the JSON object."
        )
        # Truncate text to fit in context window (increase limit if possible)
        truncated_text = raw_text[:16000]

        user_prompt = (
            "Extract the following sections from this research paper and return as a flat JSON object. "
            "The JSON must have ONLY these keys: 'abstract', 'methodology', 'experiments', 'results'. "
            "Do NOT use a 'sections' array or any other structure. "
            "Each section should be a full, detailed paragraph or set of paragraphs, not a summary. "
            "Do not truncate or omit information. "
            "If a section is long, include as much as possible. "
            "Return ONLY a JSON object (no markdown, no explanation):\n"
            "{\"abstract\": \"text here\", \"methodology\": \"text here\", "
            "\"experiments\": \"text here\", \"results\": \"text here\"}"
        ) + f"\nPaper text:\n{truncated_text}"
        try:
            sections = self.extract_json(user_prompt, system_prompt)
            # Fallback: If sections is a dict with a 'sections' key, try to extract from array
            if isinstance(sections, dict) and 'sections' in sections:
                flat_sections = {"abstract": "", "methodology": "", "experiments": "", "results": ""}
                for item in sections['sections']:
                    name = item.get('name', '').lower()
                    content = item.get('content', '')
                    if 'abstract' in name:
                        flat_sections['abstract'] += str(content) + '\n'
                    elif 'methodology' in name or 'approach' in name:
                        flat_sections['methodology'] += str(content) + '\n'
                    elif 'experiment' in name:
                        flat_sections['experiments'] += str(content) + '\n'
                    elif 'result' in name:
                        flat_sections['results'] += str(content) + '\n'
                return flat_sections
            # Ensure all keys exist
            default_sections = {
                "abstract": "", "methodology": "", "experiments": "", "results": ""}
            if isinstance(sections, dict):
                default_sections.update(sections)
            return default_sections
        except Exception as e:
            logger.error(f"Failed to extract sections with LLM: {e}")
            logger.debug(f"Attempting simple text extraction as fallback...")
            # Fallback: try to find sections by headers
            return self._simple_section_extraction(raw_text)

    def _simple_section_extraction(self, text: str) -> dict:
        """Simple regex-based section extraction as fallback."""
        import re
        sections = {"abstract": "", "methodology": "",
                    "experiments": "", "results": ""}

        # Try to find abstract
        abstract_match = re.search(
            r'(?:Abstract|ABSTRACT)[:\s]+(.*?)(?=\n\n|\n[A-Z])', text, re.DOTALL)
        if abstract_match:
            sections['abstract'] = abstract_match.group(1).strip()[:1000]

        return sections

    def _extract_metrics_from_nested_dict(self, data: dict, prefix: str = "") -> dict:
        """
        Recursively extract numeric metrics from nested dictionaries.

        Args:
            data: Nested dictionary (e.g., from complete_results.json)
            prefix: Prefix for metric names

        Returns:
            Flat dictionary of metric_name: value
        """
        metrics = {}

        for key, value in data.items():
            current_key = f"{prefix}/{key}" if prefix else key

            if isinstance(value, dict):
                # Check if this dict contains metrics
                if 'metrics' in value:
                    # Extract metrics from this level
                    metric_dict = value['metrics']
                    for metric_name, metric_value in metric_dict.items():
                        if isinstance(metric_value, dict):
                            # Handle metrics with multiple thresholds (e.g., recall@1, recall@5)
                            for threshold, val in metric_value.items():
                                if isinstance(val, (int, float)):
                                    metrics[f"{metric_name}@{threshold}"] = float(
                                        val)
                        elif isinstance(metric_value, (int, float)):
                            metrics[metric_name] = float(metric_value)
                else:
                    # Recurse into nested dicts
                    nested = self._extract_metrics_from_nested_dict(
                        value, current_key)
                    metrics.update(nested)
            elif isinstance(value, (int, float)) and not isinstance(value, bool):
                # Direct numeric value
                metrics[current_key] = float(value)

        return metrics

    def _run_experiments_unified(self, paper_content: PaperContent,
                                 codebase_info: CodebaseInfo) -> list:
        """Run experiments using the unified experiment executor (Stage 3)."""
        # Set up environment
        logger.info("Setting up experiment environment...")
        setup_success = self.experiment_executor.setup_environment(
            codebase_info.path,
            codebase_info.dependencies
        )

        if not setup_success:
            logger.warning(
                "Environment setup had issues, proceeding anyway...")

        # Check for README instructions
        priority_scripts = []
        if codebase_info.readme_content:
            # Look for python commands in README
            python_cmds = re.findall(r'python[3]?\s+([\w_/\.]+\.py)(?:\s+(.*))?',
                                     codebase_info.readme_content, re.IGNORECASE)
            for script_name, args in python_cmds:
                script_path = codebase_info.path / script_name
                if script_path.exists():
                    logger.info(
                        f"Found priority script from README: {script_name}")
                    priority_scripts.append(
                        (script_path, args.strip().split() if args else []))

        results = []

        # Run priority scripts from README first
        for script_path, args in priority_scripts[:2]:  # Limit to 2
            logger.info(
                f"Running priority script: {script_path.name} {' '.join(args)}")

            config = ExperimentConfig(
                script_path=script_path,
                args=args,
                env_vars={},
                working_dir=codebase_info.path,
                timeout=self.config['experiment']['timeout']
            )

            result = self.experiment_executor.run_experiment(config)
            results.append(result)

            if result.success:
                logger.info(f"  ‚úì Success (duration: {result.duration:.2f}s)")
            else:
                logger.warning(f"  ‚úó Failed: {result.stderr[:200]}")

        # If no priority scripts, use entry points
        if not results:
            for entry_point in codebase_info.entry_points[:3]:  # Limit to 3
                logger.info(f"Running: {entry_point.name}")

                config = ExperimentConfig(
                    script_path=entry_point,
                    args=[],
                    env_vars={},
                    working_dir=codebase_info.path,
                    timeout=self.config['experiment']['timeout']
                )

                result = self.experiment_executor.run_experiment(config)
                results.append(result)

                if result.success:
                    logger.info(
                        f"  ‚úì Success (duration: {result.duration:.2f}s)")
                else:
                    logger.warning(f"  ‚úó Failed: {result.stderr[:200]}")

        return [r for r in results if r.success]

    def _save_results(self, paper_path: Path, comparisons: list,
                      report: str, summary_stats: str, analysis: str,
                      conclusions: str, experiment_sets: list):
        """Save comprehensive results with full execution log to output directory."""
        output_file = self.output_dir / f"{paper_path.stem}_results.txt"
        log_file = Path('./outputs/agent_execution.log')

        with open(output_file, 'w', encoding='utf-8') as f:
            # Header
            f.write("="*100 + "\n")
            f.write(
                "EVALLab: RESEARCH PAPER REPRODUCTION AGENT - COMPLETE EXECUTION LOG\n")
            f.write("="*100 + "\n\n")

            f.write(f"Paper: {paper_path.name}\n")
            f.write(f"Experiment Sets Analyzed: {len(experiment_sets)}\n")
            f.write(f"Total Metric Comparisons: {len(comparisons)}\n")
            f.write(f"Generated: {time.strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write("\n" + "="*100 + "\n\n")

            # Section 1: Full Execution Log
            f.write("‚ïî" + "‚ïê"*98 + "‚ïó\n")
            f.write("‚ïë" + " SECTION 1: COMPLETE EXECUTION LOG ".center(98) + "‚ïë\n")
            f.write("‚ïö" + "‚ïê"*98 + "‚ïù\n\n")

            if log_file.exists():
                with open(log_file, 'r', encoding='utf-8') as log:
                    f.write(log.read())
            else:
                f.write("[Log file not found]\n")

            f.write("\n" + "="*100 + "\n\n")

            # Section 2: Detailed Results
            f.write("‚ïî" + "‚ïê"*98 + "‚ïó\n")
            f.write(
                "‚ïë" + " SECTION 2: REPRODUCTION RESULTS EVALUATION ".center(98) + "‚ïë\n")
            f.write("‚ïö" + "‚ïê"*98 + "‚ïù\n\n")
            f.write(report + "\n\n")

            # Section 3: Summary Statistics
            f.write("‚ïî" + "‚ïê"*98 + "‚ïó\n")
            f.write("‚ïë" + " SECTION 3: SUMMARY STATISTICS ".center(98) + "‚ïë\n")
            f.write("‚ïö" + "‚ïê"*98 + "‚ïù\n\n")
            f.write(summary_stats + "\n\n")

            # Section 4: LLM Analysis
            if analysis:
                f.write("‚ïî" + "‚ïê"*98 + "‚ïó\n")
                f.write("‚ïë" + " SECTION 4: LLM ANALYSIS ".center(98) + "‚ïë\n")
                f.write("‚ïö" + "‚ïê"*98 + "‚ïù\n\n")
                f.write(analysis + "\n\n")

            # Section 5: Conclusions
            if conclusions:
                f.write("‚ïî" + "‚ïê"*98 + "‚ïó\n")
                f.write(
                    "‚ïë" + " SECTION 5: CONCLUSIONS & RECOMMENDATIONS ".center(98) + "‚ïë\n")
                f.write("‚ïö" + "‚ïê"*98 + "‚ïù\n\n")
                f.write(conclusions + "\n\n")

            # Footer
            f.write("="*100 + "\n")
            f.write("END OF REPORT\n")
            f.write("="*100 + "\n")

        logger.info(f"‚úì Results saved to: {output_file}")
        logger.info(f"‚úì Execution log saved to: {log_file}")


def main():
    """Main entry point for CLI usage."""
    import argparse

    parser = argparse.ArgumentParser(
        description='Local Research Paper Reproduction Agent - Auto-detects paper and code from workspace'
    )
    parser.add_argument(
        'paper_path',
        nargs='?',  # Make optional
        type=Path,
        help='Path to research paper PDF (default: auto-detect from ./papers/)'
    )
    parser.add_argument(
        '--codebase',
        type=str,
        help='GitHub URL or local path to codebase (default: auto-detect from ./papers/codebases/)'
    )
    parser.add_argument(
        '--config',
        type=Path,
        help='Path to config.yaml file'
    )

    args = parser.parse_args()

    # Validate paper path if provided
    if args.paper_path and not args.paper_path.exists():
        print(f"Error: Paper file not found: {args.paper_path}")
        sys.exit(1)

    # Create and run agent
    agent = ReproductionAgent(config_path=args.config)
    results = agent.run(args.paper_path, args.codebase)

    if 'error' in results:
        print(f"Error: {results['error']}")
        sys.exit(1)


if __name__ == '__main__':
    main()
